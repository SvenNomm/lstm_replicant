{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import preprocessing_module as ppm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "katse_nr = 3\n",
    "order = 2\n",
    "dx = 5\n",
    "\n",
    "path = '/Users/svennomm/kohalikTree/Data/AIRSCS/wave/data_v2/'\n",
    "winx=256"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "path = path + \"katse_0\" + str(katse_nr) + \"/\"\n",
    "initial_data_file_1 = path + 'sarspec_hgh_order_' + str(order) + '_winx_' + str(winx) + '_co_clean.csv'\n",
    "initial_data_file_2 = path + 'sarspec_hgh_order_' + str(order) + '_winx_' + str(winx) + '_cro_clean.csv'\n",
    "target_data_file = path + 'wavespec_hgh_order_' + str(order) + '_winx_' + str(winx) + '_clean.csv'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "initial_data_1 = pd.read_csv(initial_data_file_1, sep=',')\n",
    "initial_data_2 = pd.read_csv(initial_data_file_2, sep=',')\n",
    "target_data = pd.read_csv(target_data_file, sep=',')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "initial_data_1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "initial_data_2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "input_data_1_train, input_data_2_train, target_data_train, input_data_1_test, input_data_2_test, target_data_test,\\\n",
    "        test_data_indexes = ppm.time_based_splitter(initial_data_1, initial_data_2, target_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_data_1_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_data_2_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target_data_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_data_1_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_data_1_train, input_data_2_train, target_data_train = ppm.initial_formatting_old_data(input_data_1_train, input_data_2_train, target_data_train)\n",
    "input_data_1_test, input_data_2_test, target_data_test = ppm.initial_formatting_old_data(input_data_1_test,input_data_2_test, target_data_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_data_1_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "device = torch.device('mps')\n",
    "input_tensor_1_train = torch.tensor(input_data_1_train.values).float()\n",
    "input_tensor_2_train = torch.tensor(input_data_2_train.values).float()\n",
    "target_tensor_train = torch.tensor(target_data_train.values).float()\n",
    "\n",
    "input_tensor_1_test = torch.tensor(input_data_1_test.values).float()\n",
    "input_tensor_2_test = torch.tensor(input_data_2_test.values).float()\n",
    "target_tensor_test = torch.tensor(target_data_test.values).float()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3868, 69]) torch.Size([3868, 69]) torch.Size([3868, 69])\n",
      "torch.Size([74, 69]) torch.Size([74, 69]) torch.Size([74, 69])\n"
     ]
    }
   ],
   "source": [
    "print(torch.Tensor.size(input_tensor_1_train),torch.Tensor.size(input_tensor_2_train), torch.Tensor.size(target_tensor_train))\n",
    "print(torch.Tensor.size(input_tensor_1_test),torch.Tensor.size(input_tensor_2_test), torch.Tensor.size(target_tensor_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "input_tensor_train_log_div_norm = ppm.combine_polarisations_log_div_norm(input_tensor_1_train, input_tensor_2_train)\n",
    "input_tensor_train_div_norm = ppm.combine_polarisations_div_norm(input_tensor_1_train, input_tensor_2_train)\n",
    "input_tensor_train_div = ppm.combine_polarisations_div(input_tensor_1_train, input_tensor_2_train)\n",
    "\n",
    "input_tensor_test_log_div_norm = ppm.combine_polarisations_log_div_norm(input_tensor_1_test, input_tensor_2_test)\n",
    "input_tensor_test_div_norm = ppm.combine_polarisations_div_norm(input_tensor_1_test, input_tensor_2_test)\n",
    "input_tensor_test_div = ppm.combine_polarisations_div(input_tensor_1_test, input_tensor_2_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3868, 69]) torch.Size([3868, 69]) torch.Size([3868, 69])\n",
      "torch.Size([74, 69]) torch.Size([74, 69]) torch.Size([74, 69])\n"
     ]
    }
   ],
   "source": [
    "print(torch.Tensor.size(input_tensor_train_div),torch.Tensor.size(input_tensor_train_div_norm), torch.Tensor.size(input_tensor_train_log_div_norm))\n",
    "print(torch.Tensor.size(input_tensor_test_div),torch.Tensor.size(input_tensor_test_div_norm), torch.Tensor.size(input_tensor_test_log_div_norm))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "input_data_train_log_div_norm = input_tensor_train_log_div_norm.detach().cpu().numpy()\n",
    "input_data_train_div_norm = input_tensor_train_div_norm.detach().cpu().numpy()\n",
    "input_data_train_div = input_tensor_train_div.detach().cpu().numpy()\n",
    "\n",
    "target_data_train = target_tensor_train.detach().cpu().numpy()\n",
    "input_data_1_train = input_tensor_1_train.detach().cpu().numpy()\n",
    "input_data_2_train = input_tensor_2_train.detach().cpu().numpy()\n",
    "\n",
    "input_data_test_log_div_norm = input_tensor_test_log_div_norm.detach().cpu().numpy()\n",
    "input_data_test_div_norm = input_tensor_test_div_norm.detach().cpu().numpy()\n",
    "input_data_test_div = input_tensor_test_div.detach().cpu().numpy()\n",
    "target_data_test = target_tensor_test.detach().cpu().numpy()\n",
    "input_data_1_test = input_tensor_1_test.detach().cpu().numpy()\n",
    "input_data_2_test = input_tensor_2_test.detach().cpu().numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3868, 69]) torch.Size([3868, 69]) torch.Size([3868, 69])\n",
      "torch.Size([74, 69]) torch.Size([74, 69]) torch.Size([74, 69])\n"
     ]
    }
   ],
   "source": [
    "print(torch.Tensor.size(input_tensor_train_div),torch.Tensor.size(input_tensor_train_div_norm), torch.Tensor.size(input_tensor_train_log_div_norm))\n",
    "print(torch.Tensor.size(input_tensor_test_div),torch.Tensor.size(input_tensor_test_div_norm), torch.Tensor.size(input_tensor_test_log_div_norm))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "input_data_train_log_div_norm_trunc = ppm.cut_tail(input_data_train_log_div_norm, 60)\n",
    "input_data_train_div_norm_trunc = ppm.cut_tail(input_data_train_div_norm, 60)\n",
    "input_data_train_div_trunc = ppm.cut_tail(input_data_train_div, 60)\n",
    "target_data_train_trunc = ppm.cut_tail(target_data_train, 60)\n",
    "\n",
    "input_data_test_log_div_norm_trunc = ppm.cut_tail(input_data_test_log_div_norm, 60)\n",
    "input_data_test_div_norm_trunc = ppm.cut_tail(input_data_test_div_norm, 60)\n",
    "input_data_test_div_trunc = ppm.cut_tail(input_data_test_div, 60)\n",
    "target_data_test_trunc = ppm.cut_tail(target_data_test, 60)\n",
    "\n",
    "input_data_1_train_trunc = ppm.cut_tail(input_data_1_train, 60)\n",
    "input_data_2_train_trunc = ppm.cut_tail(input_data_2_train, 60)\n",
    "input_data_1_test_trunc = ppm.cut_tail(input_data_1_test, 60)\n",
    "input_data_2_test_trunc = ppm.cut_tail(input_data_2_test, 60)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (3868, 69)\n",
      "<class 'numpy.ndarray'> (3868, 69)\n",
      "<class 'numpy.ndarray'> (3868, 69)\n",
      "<class 'numpy.ndarray'> (3868, 69)\n",
      "<class 'numpy.ndarray'> (74, 69)\n",
      "<class 'numpy.ndarray'> (74, 69)\n",
      "<class 'numpy.ndarray'> (74, 69)\n",
      "<class 'numpy.ndarray'> (74, 69)\n"
     ]
    }
   ],
   "source": [
    "print(type(input_data_train_div), input_data_train_div.shape)\n",
    "print(type(input_data_train_div_norm), input_data_train_div_norm.shape)\n",
    "print(type(input_data_train_log_div_norm), input_data_train_log_div_norm.shape)\n",
    "print(type(target_data_train), target_data_train.shape)\n",
    "\n",
    "print(type(input_data_test_div), input_data_test_div.shape)\n",
    "print(type(input_data_test_div_norm), input_data_test_div_norm.shape)\n",
    "print(type(input_data_test_log_div_norm), input_data_test_log_div_norm.shape)\n",
    "print(type(target_data_test), target_data_test.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "#generate file name\n",
    "path = '/Users/svennomm/kohalikTree/Data/AIRSCS/wave/data_v2/'\n",
    "path_1 = path + 'alternative_processing/' + \"katse_0\" + str(katse_nr) + '/'\n",
    "fname_train = path_1 + \"katse_0\" + str(katse_nr) + '_winx_' + str(winx) + '_ldn' + '.pkl'\n",
    "\n",
    "data = [input_data_train_log_div_norm,  input_data_test_log_div_norm, target_data_train, target_data_test, test_data_indexes]\n",
    "with open(fname_train, 'wb') as f:\n",
    "    pkl.dump(data, f)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "fname_train = path_1 + \"katse_0\" + str(katse_nr) + '_winx_' + str(winx) + '_dn' + '.pkl'\n",
    "\n",
    "data = [input_data_train_div_norm, input_data_test_div_norm, target_data_train, target_data_test, test_data_indexes]\n",
    "with open(fname_train, 'wb') as f:\n",
    "    pkl.dump(data, f)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "fname_train = path_1 + \"katse_0\" + str(katse_nr) + '_winx_' + str(winx) + '_ldnt' + '.pkl'\n",
    "\n",
    "data = [input_data_train_log_div_norm_trunc, input_data_test_log_div_norm_trunc, target_data_train_trunc, target_data_test_trunc, test_data_indexes]\n",
    "with open(fname_train, 'wb') as f:\n",
    "    pkl.dump(data, f)\n",
    "    f.close()\n",
    "\n",
    "fname_train = path_1 + \"katse_0\" + str(katse_nr) + '_winx_' + str(winx) + '_dnt' + '.pkl'\n",
    "\n",
    "\n",
    "data = [input_data_train_div_norm_trunc, input_data_test_div_norm_trunc, target_data_train_trunc, target_data_test_trunc, test_data_indexes]\n",
    "with open(fname_train, 'wb') as f:\n",
    "    pkl.dump(data, f)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "fname_train = path_1 + \"katse_0\" + str(katse_nr) + '_winx_' + str(winx) + '_d' + '.pkl'\n",
    "\n",
    "\n",
    "data = [input_data_train_div, input_data_test_div, target_data_train, target_data_test, test_data_indexes]\n",
    "with open(fname_train, 'wb') as f:\n",
    "    pkl.dump(data, f)\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "fname_train = path_1 + \"katse_0\" + str(katse_nr) + '_winx_' + str(winx) + '_dt' + '.pkl'\n",
    "\n",
    "\n",
    "data = [input_data_train_div_trunc, input_data_test_div_trunc, target_data_train_trunc, target_data_test_trunc, test_data_indexes]\n",
    "with open(fname_train, 'wb') as f:\n",
    "    pkl.dump(data, f)\n",
    "    f.close()\n",
    "\n",
    "fname_train = path_1 + \"katse_0\" + str(katse_nr) + '_winx_' + str(winx) + '_o' + '.pkl'\n",
    "\n",
    "\n",
    "data = [input_data_1_train, input_data_1_test, input_data_2_train, input_data_2_test, target_data_train, target_data_test, test_data_indexes]\n",
    "with open(fname_train, 'wb') as f:\n",
    "    pkl.dump(data, f)\n",
    "    f.close()\n",
    "\n",
    "fname_train = path_1 + \"katse_0\" + str(katse_nr) + '_winx_' + str(winx) + '_ot' + '.pkl'\n",
    "\n",
    "\n",
    "data = [input_data_1_train_trunc, input_data_1_test_trunc, input_data_2_train_trunc, input_data_2_test_trunc, target_data_train_trunc, target_data_test_trunc, test_data_indexes]\n",
    "with open(fname_train, 'wb') as f:\n",
    "    pkl.dump(data, f)\n",
    "    f.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
